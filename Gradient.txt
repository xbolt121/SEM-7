import matplotlib as plot
import numpy as np
import sympy as sym #Lib for Symbolic Math
from matplotlib import pyplot
def objective(x):
return (x+3)**2
def derivative(x):
return 2*(x + 3)
def gradient_descent(alpha, start, max_iter):
x_list = list()
x= start;
x_list.append(x)
for i in range(max_iter):
gradient = derivative(x);
x = x - (alpha*gradient);
x_list.append(x);
return x_list
x = sym.symbols('x')
expr = (x+3)**2.0;
grad = sym.Derivative(expr,x)
print("{}".format(grad.doit()) )
grad.doit().subs(x,2)
2.0*(x + 3)**1.0
$\displaystyle 10.0$
def gradient_descent1(expr,alpha, start, max_iter):
x_list = list()
x = sym.symbols('x')
grad = sym.Derivative(expr,x).doit()
x_val= start;
x_list.append(x_val)
for i in range(max_iter):
gradient = grad.subs(x,x_val);
x_val = x_val - (alpha*gradient);
x_list.append(x_val);
return x_list
alpha = 0.1 #Step_size
start = 2 #Starting point
max_iter = 50 #Limit on iterations
x = sym.symbols('x')
expr = (x+3)**2; #target function
x_cordinate = np.linspace(-15,15,100)
pyplot.plot(x_cordinate,objective(x_cordinate))
pyplot.plot(2,objective(2),'ro')
[<matplotlib.lines.Line2D at 0x22b63298b10>]

In [1]: In [2]: In [3]: In [4]: In [5]: Out[5]: In [6]: In [15]: In [16]: Out[16]:
9/25/24, 9:31 AM exp-4

localhost:8888/lab/tree/Downloads/exp-4.ipynb 1/3

X = gradient_descent(alpha,start,max_iter)
x_cordinate = np.linspace(-5,5,100)
pyplot.plot(x_cordinate,objective(x_cordinate))
X_arr = np.array(X)
pyplot.plot(X_arr, objective(X_arr), '.-', color='red')
pyplot.axvline(-3,color='green',lw=1,ls='--',label="minimum at x=-3")
pyplot.show()